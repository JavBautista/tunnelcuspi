# PROMPT COMPLETO PARA CLAUDE - PROYECTO CUSPI (DIGITAL OCEAN)

## üéØ CONTEXTO DEL SISTEMA

Eres Claude trabajando en el proyecto **CUSPI** (Digital Ocean). Tu tarea es implementar un sistema de sincronizaci√≥n autom√°tica que descarga backups de base de datos desde **TunnelCUSPI** (servidor Windows SICAR) e importa los datos a tu base de datos PostgreSQL en Digital Ocean.

### üìã ARQUITECTURA GENERAL
```
TunnelCUSPI (Windows SICAR) ‚Üê‚Üí CUSPI (Digital Ocean)
         Laravel 8                    Django/FastAPI/etc
         MySQL BD                     PostgreSQL BD
         Puerto 3307                  Puerto 5432
```

## üîó SERVIDOR TUNNEL (YA IMPLEMENTADO)

### **URL Base**: `https://tunnelcuspi.site` (HTTP) o `http://tunnelcuspi.site` (fallback)
### **Autenticaci√≥n**: `API-KEY: uN4gFh7!rT3@kLp98#Qwz`
### **Status**: ‚úÖ PROBADO Y FUNCIONAL (Agosto 2025)

### **Endpoints Disponibles**:

#### 1. **POST /api/backup/full** - Solicitar Backup

### **MODO A: Webhook (Asincr√≥nico) - RECOMENDADO ‚úÖ**
```bash
curl -X POST http://tunnelcuspi.site/api/backup/full \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  -H "Content-Type: application/json" \
  -d '{"webhook_url": "https://cuspi.do/webhook/backup-ready"}'
```

**Respuesta Inmediata** (~1 segundo):
```json
{
  "ok": true,
  "type": "webhook",
  "job_id": "backup_68a7d15a36be4",
  "status": "processing", 
  "message": "Backup iniciado en background",
  "webhook_url": "https://cuspi.do/webhook/backup-ready",
  "timestamp": "2025-08-22 02:12:36"
}
```

**Webhook Enviado** (~3 minutos despu√©s):
```bash
# TunnelCUSPI env√≠a POST a https://cuspi.do/webhook/backup-ready
POST https://cuspi.do/webhook/backup-ready
Content-Type: application/json
User-Agent: TunnelCUSPI-Webhook/1.0

# CASO √âXITO:
{
  "backup_id": "backup_68a7d15a36be4",
  "status": "completed",
  "download_url": "http://tunnelcuspi.site/api/backup/download/backup_68a7d15a36be4_sicar_backup_2025-08-22_02-48-09.sql.gz",
  "expires_at": "2025-08-22 08:51:33",
  "file_size_mb": 378.61,
  "compression_ratio": "32.8%"
}

# CASO ERROR:
{
  "backup_id": "backup_68a7d15a36be4",
  "status": "failed", 
  "error": "Error comprimiendo backup"
}
```

### **MODO B: Sincr√≥nico (Compatibilidad) ‚úÖ**  
```bash
curl -X POST http://tunnelcuspi.site/api/backup/full \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  -H "Content-Type: application/json" \
  -d "{}"
```

**Respuesta Exitosa** (~3 minutos 25 segundos):
```json
{
  "ok": true,
  "type": "download_url", 
  "download_url": "http://tunnelcuspi.site/api/backup/download/backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz",
  "filename": "backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz",
  "filesize": 396998348,
  "filesize_mb": 378.61,
  "expires_at": "2025-08-22 08:51:33",
  "timestamp": "2025-08-22_02-48-09",
  "compression": "gzip",
  "platform": "windows",
  "log_id": "backup_68a7da68aabff"
}
```

**Respuesta de Error**:
```json
{
  "ok": false,
  "error": "Error description",
  "platform": "windows",
  "log_id": "backup_12345"
}
```

#### 2. **GET /api/backup/download/{filename}** - Descargar Archivo ‚úÖ
```bash
curl -X GET http://tunnelcuspi.site/api/backup/download/backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  --output backup.sql.gz
```

**Respuesta**: Archivo binario comprimido gzip (~379MB)
**Tiempo descarga**: ~2-5 minutos (dependiendo conexi√≥n)
**Expiraci√≥n**: 6 horas desde generaci√≥n

#### 3. **üîÑ Sistema de Limpieza Autom√°tica**
- **Autom√°tico**: Mantiene m√°ximo 2 backups m√°s recientes
- **Al generar**: Elimina autom√°ticamente backups antiguos 
- **Logs**: Registra archivos eliminados con tama√±o

**Ejemplo de logs**:
```json
{"message": "ARCHIVO ANTIGUO ELIMINADO", "filename": "backup_old.sql.gz", "size_mb": 378.61}
```

#### 4. **‚ö° Retry Logic del Webhook**
- **Intentos**: 3 intentos autom√°ticos
- **Backoff**: Exponencial (1s, 4s, 16s)
- **Timeout**: 30 segundos por intento
- **Logs**: Registra cada intento y resultado final

## ‚úÖ RESULTADOS DE PRUEBAS REALES

### **Prueba Exitosa - Agosto 2025**
```json
{
  "test_date": "2025-08-22",
  "modo_sincronico": {
    "status": "‚úÖ EXITOSO",
    "duracion": "3 minutos 25 segundos",
    "archivo_generado": "backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz",
    "tama√±o_original": "1155.15 MB",
    "tama√±o_comprimido": "378.61 MB", 
    "compresion": "32.8%",
    "download_url": "http://tunnelcuspi.site/api/backup/download/backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz"
  },
  "limpieza_automatica": {
    "status": "‚úÖ EXITOSO",
    "archivos_eliminados": 2,
    "storage_management": "Solo 2 backups mantenidos"
  },
  "performance": {
    "mysqldump_time": "103.2s",
    "compression_time": "101.41s", 
    "total_time": "205.18s"
  }
}
```

## üéØ TU MISI√ìN EN CUSPI (DO)

### **TAREA PRINCIPAL**: Implementar sistema de sincronizaci√≥n autom√°tica

### **COMPONENTES A DESARROLLAR**:

#### 1. **M√≥dulo de Backup Downloader**
```python
class SicarBackupDownloader:
    def __init__(self):
        self.api_key = "uN4gFh7!rT3@kLp98#Qwz"
        self.base_url = "https://tunnelcuspi.site"
        
    def request_backup_webhook(self, webhook_url):
        """Solicita backup con webhook (RECOMENDADO)"""
        # POST /api/backup/full con {"webhook_url": "..."}
        # Retorna inmediato: {"job_id": "...", "status": "processing"}
        
    def request_backup_sync(self):
        """Solicita backup sincr√≥nico (compatibilidad)"""
        # POST /api/backup/full sin webhook_url
        # Retorna despu√©s de 3min: {"download_url": "..."}
        
    def download_backup(self, download_url):
        """Descarga archivo comprimido desde URL"""
        # GET /api/backup/download/{filename}
        # Retorna: archivo .sql.gz
        
    def verify_backup(self, filepath):
        """Verifica integridad del archivo descargado"""
        # Verificar tama√±o, formato, puede descomprimir
```

#### **Webhook Handler (NUEVO)**
```python
class WebhookHandler:
    def handle_backup_ready(self, webhook_data):
        """Maneja webhook cuando backup est√° listo"""
        # webhook_data = {
        #   "backup_id": "backup_68a7d15a36be4",
        #   "status": "completed", 
        #   "download_url": "http://tunnelcuspi.site/api/backup/download/file.sql.gz",
        #   "expires_at": "2025-08-22 08:51:33",
        #   "file_size_mb": 378.61,
        #   "compression_ratio": "32.8%"
        # }
        
    def handle_backup_failed(self, webhook_data):
        """Maneja webhook cuando backup fall√≥"""
        # webhook_data = {
        #   "backup_id": "backup_68a7d15a36be4",
        #   "status": "failed",
        #   "error": "Error comprimiendo backup"
        # }
        
    @app.route('/webhook/backup-ready', methods=['POST'])
    def webhook_endpoint(self):
        """Endpoint para recibir webhooks de TunnelCUSPI"""
        # DEBE responder {"ok": True} para confirmar recepci√≥n
        # TunnelCUSPI reintentar√° 3 veces si no recibe 200 OK
        data = request.get_json()
        
        if data['status'] == 'completed':
            # Iniciar descarga e importaci√≥n autom√°tica
            process_backup_async.delay(data)
        elif data['status'] == 'failed':
            # Manejar error y notificar
            handle_backup_error(data['error'])
            
        return {"ok": True}
```

#### 2. **M√≥dulo de Procesamiento de Datos**
```python
class BackupProcessor:
    def decompress_backup(self, gz_filepath):
        """Descomprime archivo .gz a .sql"""
        
    def parse_mysql_dump(self, sql_filepath):
        """Convierte MySQL dump a comandos PostgreSQL"""
        # IMPORTANTE: Convertir sintaxis MySQL ‚Üí PostgreSQL
        # - AUTO_INCREMENT ‚Üí SERIAL
        # - MyISAM ‚Üí InnoDB adaptations
        # - DATE/DATETIME formats
        # - Escape characters
        
    def import_to_postgres(self, processed_sql):
        """Importa datos a PostgreSQL"""
        # Conectar a BD PostgreSQL local
        # Ejecutar comandos SQL procesados
        # Manejar errores de importaci√≥n
```

#### 3. **Scheduler/Cronjob**
```python
class BackupScheduler:
    def run_sync(self):
        """Ejecuta sincronizaci√≥n completa"""
        # 1. Solicitar backup
        # 2. Descargar archivo
        # 3. Procesar e importar
        # 4. Logging y notificaciones
        # 5. Limpieza de archivos temporales
        
    def setup_cron(self):
        """Configura ejecuci√≥n autom√°tica"""
        # Cada 12 horas: 6:00 AM / 6:00 PM
```

#### 4. **Sistema de Logging y Monitoreo**
```python
class SyncLogger:
    def log_sync_start(self):
        """Log inicio de sincronizaci√≥n"""
        
    def log_download_progress(self, progress):
        """Log progreso de descarga"""
        
    def log_import_results(self, stats):
        """Log resultados de importaci√≥n"""
        
    def send_notifications(self, status, details):
        """Enviar notificaciones de √©xito/error"""
```

## üìä ESTRUCTURA DE DATOS SICAR

### **Tablas Principales**:
```sql
-- Tabla: articulo (productos)
CREATE TABLE articulo (
    art_id INT PRIMARY KEY AUTO_INCREMENT,
    clave VARCHAR(50),
    claveAlterna VARCHAR(50),
    descripcion TEXT,
    servicio TINYINT,
    localizacion VARCHAR(100),
    caracteristicas TEXT,
    margen1 DECIMAL(10,2),
    margen2 DECIMAL(10,2),
    margen3 DECIMAL(10,2),
    margen4 DECIMAL(10,2),
    precio1 DECIMAL(10,2),
    precio2 DECIMAL(10,2),
    precio3 DECIMAL(10,2),
    precio4 DECIMAL(10,2),
    mayoreo1 DECIMAL(10,2),
    mayoreo2 DECIMAL(10,2),
    mayoreo3 DECIMAL(10,2),
    mayoreo4 DECIMAL(10,2),
    invMin DECIMAL(10,2),
    invMax DECIMAL(10,2),
    existencia DECIMAL(10,2),
    status TINYINT,
    factor DECIMAL(10,4),
    precioCompra DECIMAL(10,2),
    preCompraProm DECIMAL(10,2),
    unidadCompra VARCHAR(20),
    unidadVenta VARCHAR(20),
    cuentaPredial VARCHAR(50),
    cat_id INT
);

-- Tabla: categoria
CREATE TABLE categoria (
    cat_id INT PRIMARY KEY AUTO_INCREMENT,
    nombre VARCHAR(100),
    system TINYINT,
    status TINYINT,
    dep_id INT,
    comision DECIMAL(5,2)
);

-- Tabla: departamento
CREATE TABLE departamento (
    dep_id INT PRIMARY KEY AUTO_INCREMENT,
    nombre VARCHAR(100),
    restringido TINYINT,
    porcentaje DECIMAL(5,2),
    system TINYINT,
    status TINYINT,
    comision DECIMAL(5,2)
);
```

## üîß CONFIGURACI√ìN REQUERIDA

### **Variables de Entorno**:
```env
# TunnelCUSPI API
TUNNEL_API_URL=https://tunnelcuspi.site
TUNNEL_API_KEY=uN4gFh7!rT3@kLp98#Qwz

# PostgreSQL Local
DATABASE_URL=postgresql://user:password@localhost:5432/cuspi_db

# Configuraci√≥n Sync
SYNC_INTERVAL_HOURS=12
BACKUP_RETENTION_DAYS=7
NOTIFICATION_EMAIL=admin@cuspi.com
LOG_LEVEL=INFO
```

### **Dependencias Python**:
```txt
requests>=2.31.0
psycopg2-binary>=2.9.0
schedule>=1.2.0
python-decouple>=3.8
sqlparse>=0.4.0
gzip
logging
```

## ‚ö†Ô∏è CONSIDERACIONES CR√çTICAS

### **1. Manejo de Errores**:
- **Timeout de descarga**: Archivo 400MB puede tardar varios minutos
- **Espacio en disco**: Verificar espacio disponible antes de descargar
- **Conexi√≥n de red**: Implementar reintentos con backoff exponencial
- **Errores de importaci√≥n**: Rollback en caso de falla parcial

### **2. Conversi√≥n MySQL ‚Üí PostgreSQL**:
```sql
-- Ejemplos de conversiones necesarias:
-- MySQL: AUTO_INCREMENT
-- PostgreSQL: SERIAL o IDENTITY

-- MySQL: `column`
-- PostgreSQL: "column"

-- MySQL: DATE '2025-01-01'
-- PostgreSQL: DATE '2025-01-01' (igual)

-- MySQL: TINYINT
-- PostgreSQL: SMALLINT
```

### **3. Rendimiento**:
- **Batch inserts**: Insertar en lotes de 1000 registros
- **√çndices**: Crear √≠ndices despu√©s de importaci√≥n
- **Transacciones**: Usar transacciones para integridad
- **Vacuum**: Ejecutar VACUUM ANALYZE despu√©s de importaci√≥n

### **4. Seguridad**:
- **API Key**: Nunca hardcodear en c√≥digo
- **Conexiones**: Usar SSL para conexiones BD
- **Archivos temporales**: Eliminar despu√©s de uso
- **Logs**: No logear datos sensibles

## üöÄ FLUJO DE EJECUCI√ìN COMPLETO

### **Secuencia de Pasos WEBHOOK (RECOMENDADO)**:
```python
# PASO 1: Solicitar backup con webhook
def request_sicar_backup():
    logger.info("üöÄ SOLICITANDO BACKUP SICAR")
    
    webhook_url = "https://cuspi.do/webhook/backup-ready"
    response = downloader.request_backup_webhook(webhook_url)
    
    logger.info(f"üìã Job ID: {response['job_id']}")
    return response['job_id']

# PASO 2: Endpoint webhook (autom√°tico)
@app.route('/webhook/backup-ready', methods=['POST'])
def webhook_backup_ready():
    webhook_data = request.get_json()
    
    if webhook_data['status'] == 'ready':
        # Procesar backup autom√°ticamente
        process_backup_async.delay(webhook_data)
        return {"ok": True, "message": "Backup processing started"}
    
    elif webhook_data['status'] == 'failed':
        logger.error(f"‚ùå BACKUP FALL√ì: {webhook_data['error']}")
        send_error_notification(webhook_data['error'])
        return {"ok": True, "message": "Error notification sent"}

# PASO 3: Procesamiento asincr√≥nico
def process_backup_async(webhook_data):
    logger.info("üöÄ PROCESANDO BACKUP RECIBIDO")
    job_id = webhook_data['job_id']
    
    try:
        # 1. Descargar archivo
        logger.info(f"‚¨áÔ∏è Descargando {webhook_data['filesize_mb']}MB...")
        filepath = downloader.download_backup(webhook_data['download_url'])
        
        # 2. Verificar descarga
        logger.info("üîç Verificando archivo descargado...")
        processor.verify_backup(filepath)
        
        # 3. Descomprimir
        logger.info("üìÇ Descomprimiendo backup...")
        sql_file = processor.decompress_backup(filepath)
        
        # 4. Procesar SQL
        logger.info("üîÑ Convirtiendo MySQL ‚Üí PostgreSQL...")
        processed_sql = processor.parse_mysql_dump(sql_file)
        
        # 5. Importar a PostgreSQL
        logger.info("üì• Importando datos a PostgreSQL...")
        import_stats = processor.import_to_postgres(processed_sql)
        
        # 6. Limpieza
        logger.info("üßπ Limpiando archivos temporales...")
        cleanup_temp_files([filepath, sql_file])
        
        # 7. Notificaci√≥n de √©xito
        logger.info("‚úÖ SINCRONIZACI√ìN COMPLETADA")
        send_success_notification(import_stats, job_id)
        
    except Exception as e:
        logger.error(f"‚ùå ERROR EN SINCRONIZACI√ìN: {e}")
        send_error_notification(str(e), job_id)
        raise
```

### **Secuencia SINCR√ìNICA (Alternativa)**:
```python
def sync_sicar_data_sync():
    logger.info("üöÄ INICIANDO SINCRONIZACI√ìN SICAR (SINCR√ìNICA)")
    
    try:
        # 1. Solicitar backup (espera 3 minutos)
        logger.info("üì§ Solicitando backup a TunnelCUSPI...")
        backup_info = downloader.request_backup_sync()
        
        if not backup_info['ok']:
            raise Exception(f"Error en backup: {backup_info['error']}")
            
        # 2. Continuar con descarga e importaci√≥n...
        # (resto del flujo igual)
        
    except Exception as e:
        logger.error(f"‚ùå ERROR EN SINCRONIZACI√ìN: {e}")
        send_error_notification(str(e))
        raise
```

## üìã CRITERIOS DE ACEPTACI√ìN

### **Funcionalidades Obligatorias**:
- [ ] Solicitar backup a TunnelCUSPI via API
- [ ] Descargar archivo comprimido (400MB)
- [ ] Descomprimir archivo .gz
- [ ] Convertir sintaxis MySQL ‚Üí PostgreSQL  
- [ ] Importar datos a PostgreSQL local
- [ ] Logging completo del proceso
- [ ] Manejo de errores y reintentos
- [ ] Limpieza de archivos temporales
- [ ] Notificaciones de √©xito/error
- [ ] Programaci√≥n autom√°tica (cronjob)

### **M√©tricas de √âxito**:
- **Tiempo total**: < 15 minutos end-to-end
- **√âxito de descarga**: 99.9% confiabilidad
- **Integridad de datos**: 100% registros importados
- **Disponibilidad**: Sync cada 12 horas sin fallos

## üîß COMANDOS DE TESTING

### **Probar Manualmente (COMANDOS VALIDADOS)**:
```bash
# 1. Probar solicitud de backup SINCR√ìNICO ‚úÖ
curl -X POST http://tunnelcuspi.site/api/backup/full \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  -H "Content-Type: application/json" \
  -d "{}"

# 2. Probar solicitud de backup WEBHOOK ‚úÖ  
curl -X POST http://tunnelcuspi.site/api/backup/full \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  -H "Content-Type: application/json" \
  -d '{"webhook_url": "https://tu-dominio.com/webhook/backup-ready"}'

# 3. Probar descarga (usar URL real del paso 1) ‚úÖ
curl -X GET "http://tunnelcuspi.site/api/backup/download/backup_68a7da68aabff_sicar_backup_2025-08-22_02-48-09.sql.gz" \
  -H "API-KEY: uN4gFh7!rT3@kLp98#Qwz" \
  --output test_backup.sql.gz

# 4. Verificar archivo descargado ‚úÖ
file test_backup.sql.gz
gunzip -t test_backup.sql.gz
ls -lh test_backup.sql.gz  # Debe mostrar ~379MB
```

### **Validar Datos Importados**:
```sql
-- Verificar conteos
SELECT COUNT(*) FROM articulo;
SELECT COUNT(*) FROM categoria;  
SELECT COUNT(*) FROM departamento;

-- Verificar integridad
SELECT art_id, clave, descripcion FROM articulo LIMIT 5;
```

## üìû INFORMACI√ìN ADICIONAL

### **Contacto TunnelCUSPI**:
- **URL**: http://tunnelcuspi.site (usar HTTP, no HTTPS)
- **Status**: ‚úÖ OPERATIVO Y PROBADO (Agosto 2025)
- **Backup Size**: ~379MB comprimido (~1155MB descomprimido)
- **Tiempo generaci√≥n**: ~3 minutos 25 segundos 
- **Expiraci√≥n archivos**: 6 horas
- **Performance real**:
  - MySQL dump: ~103 segundos
  - Compresi√≥n gzip: ~101 segundos  
  - Total end-to-end: ~205 segundos
- **Storage**: M√°ximo 2 backups (limpieza autom√°tica)

### **Datos de Testing**:
- **Tablas principales**: articulo (~50K registros), categoria (~200), departamento (~20)
- **Campos cr√≠ticos**: art_id, clave, descripcion, precios, existencia
- **Relaciones**: articulo.cat_id ‚Üí categoria.cat_id, categoria.dep_id ‚Üí departamento.dep_id

---

## üéØ OBJETIVO FINAL

**Implementar sistema completamente automatizado** que sincronice datos SICAR ‚Üí CUSPI cada 12 horas, con monitoreo, logging y notificaciones, manteniendo integridad de datos y alta disponibilidad.

**¬°MANOS A LA OBRA!** üöÄ